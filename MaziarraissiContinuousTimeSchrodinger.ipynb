{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install plotting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LtG0x6U89u0",
        "outputId": "a50bf2ce-63b4-4063-c357-7d53d60f1825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plotting\n",
            "  Downloading plotting-0.0.7-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from plotting) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from plotting) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from plotting) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->plotting) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->plotting) (1.16.0)\n",
            "Installing collected packages: plotting\n",
            "Successfully installed plotting-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "# from plotting import newfig\n",
        "# from plotting import savefig\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "metadata": {
        "id": "unIWSePg9MSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExternalOptimizerInterface(object):\n",
        "  \"\"\"Base class for interfaces with external optimization algorithms.\n",
        "\n",
        "  Subclass this and implement `_minimize` in order to wrap a new optimization\n",
        "  algorithm.\n",
        "\n",
        "  `ExternalOptimizerInterface` should not be instantiated directly; instead use\n",
        "  e.g. `ScipyOptimizerInterface`.\n",
        "\n",
        "  @@__init__\n",
        "\n",
        "  @@minimize\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, loss, var_list=None, equalities=None, inequalities=None,\n",
        "               **optimizer_kwargs):\n",
        "    \"\"\"Initialize a new interface instance.\n",
        "\n",
        "    Args:\n",
        "      loss: A scalar `Tensor` to be minimized.\n",
        "      var_list: Optional list of `Variable` objects to update to minimize\n",
        "        `loss`.  Defaults to the list of variables collected in the graph\n",
        "        under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
        "      equalities: Optional list of equality constraint scalar `Tensor`s to be\n",
        "        held equal to zero.\n",
        "      inequalities: Optional list of inequality constraint scalar `Tensor`s\n",
        "        to be kept nonnegative.\n",
        "      **optimizer_kwargs: Other subclass-specific keyword arguments.\n",
        "    \"\"\"\n",
        "    self._loss = loss\n",
        "    self._equalities = equalities or []\n",
        "    self._inequalities = inequalities or []\n",
        "\n",
        "    if var_list is None:\n",
        "      self._vars = variables.trainable_variables()\n",
        "    else:\n",
        "      self._vars = list(var_list)\n",
        "\n",
        "    self._update_placeholders = [array_ops.placeholder(var.dtype)\n",
        "                                 for var in self._vars]\n",
        "    self._var_updates = [var.assign(array_ops.reshape(placeholder,\n",
        "                                                      _get_shape_tuple(var)))\n",
        "                         for var, placeholder in\n",
        "                         zip(self._vars, self._update_placeholders)]\n",
        "\n",
        "    loss_grads = _compute_gradients(loss, self._vars)\n",
        "    equalities_grads = [_compute_gradients(equality, self._vars)\n",
        "                        for equality in self._equalities]\n",
        "    inequalities_grads = [_compute_gradients(inequality, self._vars)\n",
        "                          for inequality in self._inequalities]\n",
        "\n",
        "    self.optimizer_kwargs = optimizer_kwargs\n",
        "\n",
        "    self._packed_var = self._pack(self._vars)\n",
        "    self._packed_loss_grad = self._pack(loss_grads)\n",
        "    self._packed_equality_grads = [\n",
        "        self._pack(equality_grads)\n",
        "        for equality_grads in equalities_grads\n",
        "    ]\n",
        "    self._packed_inequality_grads = [\n",
        "        self._pack(inequality_grads)\n",
        "        for inequality_grads in inequalities_grads\n",
        "    ]\n",
        "\n",
        "    dims = [_prod(_get_shape_tuple(var)) for var in self._vars]\n",
        "    accumulated_dims = list(_accumulate(dims))\n",
        "    self._packing_slices = [\n",
        "        slice(start, end) for start, end in zip(accumulated_dims[:-1],\n",
        "                                                accumulated_dims[1:])]\n",
        "\n",
        "  def minimize(self,\n",
        "               session=None,\n",
        "               feed_dict=None,\n",
        "               fetches=None,\n",
        "               step_callback=None,\n",
        "               loss_callback=None,\n",
        "               **run_kwargs):\n",
        "    \"\"\"Minimize a scalar `Tensor`.\n",
        "\n",
        "    Variables subject to optimization are updated in-place at the end of\n",
        "    optimization.\n",
        "\n",
        "    Note that this method does *not* just return a minimization `Op`, unlike\n",
        "    `Optimizer.minimize()`; instead it actually performs minimization by\n",
        "    executing commands to control a `Session`.\n",
        "\n",
        "    Args:\n",
        "      session: A `Session` instance.\n",
        "      feed_dict: A feed dict to be passed to calls to `session.run`.\n",
        "      fetches: A list of `Tensor`s to fetch and supply to `loss_callback`\n",
        "        as positional arguments.\n",
        "      step_callback: A function to be called at each optimization step;\n",
        "        arguments are the current values of all optimization variables\n",
        "        flattened into a single vector.\n",
        "      loss_callback: A function to be called every time the loss and gradients\n",
        "        are computed, with evaluated fetches supplied as positional arguments.\n",
        "      **run_kwargs: kwargs to pass to `session.run`.\n",
        "    \"\"\"\n",
        "    session = session or ops.get_default_session()\n",
        "    feed_dict = feed_dict or {}\n",
        "    fetches = fetches or []\n",
        "\n",
        "    loss_callback = loss_callback or (lambda *fetches: None)\n",
        "    step_callback = step_callback or (lambda xk: None)\n",
        "\n",
        "    # Construct loss function and associated gradient.\n",
        "    loss_grad_func = self._make_eval_func(\n",
        "        [self._loss, self._packed_loss_grad],\n",
        "        session, feed_dict, fetches, loss_callback)\n",
        "\n",
        "    # Construct equality constraint functions and associated gradients.\n",
        "    equality_funcs = self._make_eval_funcs(\n",
        "        self._equalities, session, feed_dict, fetches)\n",
        "    equality_grad_funcs = self._make_eval_funcs(\n",
        "        self._packed_equality_grads, session, feed_dict, fetches)\n",
        "\n",
        "    # Construct inequality constraint functions and associated gradients.\n",
        "    inequality_funcs = self._make_eval_funcs(\n",
        "        self._inequalities, session, feed_dict, fetches)\n",
        "    inequality_grad_funcs = self._make_eval_funcs(\n",
        "        self._packed_inequality_grads, session, feed_dict, fetches)\n",
        "\n",
        "    # Get initial value from TF session.\n",
        "    initial_packed_var_val = session.run(self._packed_var)\n",
        "\n",
        "    # Perform minimization.\n",
        "    packed_var_val = self._minimize(\n",
        "        initial_val=initial_packed_var_val, loss_grad_func=loss_grad_func,\n",
        "        equality_funcs=equality_funcs,\n",
        "        equality_grad_funcs=equality_grad_funcs,\n",
        "        inequality_funcs=inequality_funcs,\n",
        "        inequality_grad_funcs=inequality_grad_funcs,\n",
        "        step_callback=step_callback, optimizer_kwargs=self.optimizer_kwargs)\n",
        "    var_vals = [packed_var_val[packing_slice]\n",
        "                for packing_slice in self._packing_slices]\n",
        "\n",
        "    # Set optimization variables to their new values.\n",
        "    session.run(\n",
        "        self._var_updates,\n",
        "        feed_dict=dict(zip(self._update_placeholders, var_vals)),\n",
        "        **run_kwargs)\n",
        "\n",
        "  def _minimize(self, initial_val, loss_grad_func, equality_funcs,\n",
        "                equality_grad_funcs, inequality_funcs, inequality_grad_funcs,\n",
        "                step_callback, optimizer_kwargs):\n",
        "    \"\"\"Wrapper for a particular optimization algorithm implementation.\n",
        "\n",
        "    It would be appropriate for a subclass implementation of this method to\n",
        "    raise `NotImplementedError` if unsupported arguments are passed: e.g. if an\n",
        "    algorithm does not support constraints but `len(equality_funcs) > 0`.\n",
        "\n",
        "    Args:\n",
        "      initial_val: A NumPy vector of initial values.\n",
        "      loss_grad_func: A function accepting a NumPy packed variable vector and\n",
        "        returning two outputs, a loss value and the gradient of that loss with\n",
        "        respect to the packed variable vector.\n",
        "      equality_funcs: A list of functions each of which specifies a scalar\n",
        "        quantity that an optimizer should hold exactly zero.\n",
        "      equality_grad_funcs: A list of gradients of equality_funcs.\n",
        "      inequality_funcs: A list of functions each of which specifies a scalar\n",
        "        quantity that an optimizer should hold >= 0.\n",
        "      inequality_grad_funcs: A list of gradients of inequality_funcs.\n",
        "      step_callback: A callback function to execute at each optimization step,\n",
        "        supplied with the current value of the packed variable vector.\n",
        "      optimizer_kwargs: Other key-value arguments available to the optimizer.\n",
        "\n",
        "    Returns:\n",
        "      The optimal variable vector as a NumPy vector.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\n",
        "        'To use ExternalOptimizerInterface, subclass from it and implement '\n",
        "        'the _minimize() method.')\n",
        "\n",
        "  @classmethod\n",
        "  def _pack(cls, tensors):\n",
        "    \"\"\"Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\"\"\"\n",
        "    if not tensors:\n",
        "      return None\n",
        "    elif len(tensors) == 1:\n",
        "      return array_ops.reshape(tensors[0], [-1])\n",
        "    else:\n",
        "      flattened = [array_ops.reshape(tensor, [-1]) for tensor in tensors]\n",
        "      return array_ops.concat(flattened, 0)\n",
        "\n",
        "  def _make_eval_func(self, tensors, session, feed_dict, fetches,\n",
        "                      callback=None):\n",
        "    \"\"\"Construct a function that evaluates a `Tensor` or list of `Tensor`s.\"\"\"\n",
        "    if not isinstance(tensors, list):\n",
        "      tensors = [tensors]\n",
        "    num_tensors = len(tensors)\n",
        "\n",
        "    def eval_func(x):\n",
        "      \"\"\"Function to evaluate a `Tensor`.\"\"\"\n",
        "      augmented_feed_dict = {\n",
        "          var: x[packing_slice].reshape(_get_shape_tuple(var))\n",
        "          for var, packing_slice in zip(self._vars, self._packing_slices)\n",
        "      }\n",
        "      augmented_feed_dict.update(feed_dict)\n",
        "      augmented_fetches = tensors + fetches\n",
        "\n",
        "      augmented_fetch_vals = session.run(\n",
        "          augmented_fetches, feed_dict=augmented_feed_dict)\n",
        "\n",
        "      if callable(callback):\n",
        "        callback(*augmented_fetch_vals[num_tensors:])\n",
        "\n",
        "      return augmented_fetch_vals[:num_tensors]\n",
        "\n",
        "    return eval_func\n",
        "\n",
        "  def _make_eval_funcs(self, tensors, session, feed_dict, fetches,\n",
        "                       callback=None):\n",
        "    return [\n",
        "        self._make_eval_func(tensor, session, feed_dict, fetches, callback)\n",
        "        for tensor in tensors\n",
        "    ]\n",
        "\n",
        "\n",
        "class ScipyOptimizerInterface(ExternalOptimizerInterface):\n",
        "  \"\"\"Wrapper allowing `scipy.optimize.minimize` to operate a `tf.Session`.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  vector = tf.Variable([7., 7.], 'vector')\n",
        "\n",
        "  # Make vector norm as small as possible.\n",
        "  loss = tf.reduce_sum(tf.square(vector))\n",
        "\n",
        "  optimizer = ScipyOptimizerInterface(loss, options={'maxiter': 100})\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    optimizer.minimize(session)\n",
        "\n",
        "  # The value of vector should now be [0., 0.].\n",
        "  ```\n",
        "\n",
        "  Example with constraints:\n",
        "\n",
        "  ```python\n",
        "  vector = tf.Variable([7., 7.], 'vector')\n",
        "\n",
        "  # Make vector norm as small as possible.\n",
        "  loss = tf.reduce_sum(tf.square(vector))\n",
        "  # Ensure the vector's y component is = 1.\n",
        "  equalities = [vector[1] - 1.]\n",
        "  # Ensure the vector's x component is >= 1.\n",
        "  inequalities = [vector[0] - 1.]\n",
        "\n",
        "  # Our default SciPy optimization algorithm, L-BFGS-B, does not support\n",
        "  # general constraints. Thus we use SLSQP instead.\n",
        "  optimizer = ScipyOptimizerInterface(\n",
        "      loss, equalities=equalities, inequalities=inequalities, method='SLSQP')\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    optimizer.minimize(session)\n",
        "\n",
        "  # The value of vector should now be [1., 1.].\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  _DEFAULT_METHOD = 'L-BFGS-B'\n",
        "\n",
        "  def _minimize(self, initial_val, loss_grad_func, equality_funcs,\n",
        "                equality_grad_funcs, inequality_funcs, inequality_grad_funcs,\n",
        "                step_callback, optimizer_kwargs):\n",
        "    def loss_grad_func_wrapper(x):\n",
        "      # SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\n",
        "      loss, gradient = loss_grad_func(x)\n",
        "      return loss, gradient.astype('float64')\n",
        "\n",
        "    method = optimizer_kwargs.pop('method', self._DEFAULT_METHOD)\n",
        "\n",
        "    constraints = []\n",
        "    for func, grad_func in zip(equality_funcs, equality_grad_funcs):\n",
        "      constraints.append({'type': 'eq', 'fun': func, 'jac': grad_func})\n",
        "    for func, grad_func in zip(inequality_funcs, inequality_grad_funcs):\n",
        "      constraints.append({'type': 'ineq', 'fun': func, 'jac': grad_func})\n",
        "\n",
        "    minimize_args = [loss_grad_func_wrapper, initial_val]\n",
        "    minimize_kwargs = {\n",
        "        'jac': True,\n",
        "        'callback': step_callback,\n",
        "        'method': method,\n",
        "        'constraints': constraints,\n",
        "    }\n",
        "    minimize_kwargs.update(optimizer_kwargs)\n",
        "    if method == 'SLSQP':\n",
        "      # SLSQP doesn't support step callbacks. Obviate associated warning\n",
        "      # message.\n",
        "      del minimize_kwargs['callback']\n",
        "\n",
        "    import scipy.optimize  # pylint: disable=g-import-not-at-top\n",
        "    result = scipy.optimize.minimize(*minimize_args, **minimize_kwargs)\n",
        "    logging.info('Optimization terminated with:\\n'\n",
        "                 '  Message: %s\\n'\n",
        "                 '  Objective function value: %f\\n'\n",
        "                 '  Number of iterations: %d\\n'\n",
        "                 '  Number of functions evaluations: %d',\n",
        "                 result.message, result.fun, result.nit, result.nfev)\n",
        "\n",
        "    return result['x']\n",
        "\n",
        "\n",
        "def _accumulate(list_):\n",
        "  total = 0\n",
        "  yield total\n",
        "  for x in list_:\n",
        "    total += x\n",
        "    yield total\n",
        "\n",
        "\n",
        "def _get_shape_tuple(tensor):\n",
        "  return tuple(dim.value for dim in tensor.get_shape())\n",
        "\n",
        "\n",
        "def _prod(array):\n",
        "  prod = 1\n",
        "  for value in array:\n",
        "    prod *= value\n",
        "  return prod\n",
        "\n",
        "\n",
        "def _compute_gradients(tensor, var_list):\n",
        "  grads = gradients.gradients(tensor, var_list)\n",
        "  # tf.gradients sometimes returns `None` when it should return 0.\n",
        "  return [grad if grad is not None else array_ops.zeros_like(var)\n",
        "          for var, grad in zip(var_list, grads)]"
      ],
      "metadata": {
        "id": "UlOcjzkUgLZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub):\n",
        "\n",
        "        X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "        X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "        X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        self.x0 = X0[:,0:1]\n",
        "        self.t0 = X0[:,1:2]\n",
        "\n",
        "        self.x_lb = X_lb[:,0:1]\n",
        "        self.t_lb = X_lb[:,1:2]\n",
        "\n",
        "        self.x_ub = X_ub[:,0:1]\n",
        "        self.t_ub = X_ub[:,1:2]\n",
        "\n",
        "        self.x_f = X_f[:,0:1]\n",
        "        self.t_f = X_f[:,1:2]\n",
        "\n",
        "        self.u0 = u0\n",
        "        self.v0 = v0\n",
        "\n",
        "        # Initialize NNs\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # tf Placeholders\n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "\n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
        "        self.v0_tf = tf.placeholder(tf.float32, shape=[None, self.v0.shape[1]])\n",
        "\n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
        "\n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
        "\n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "        # tf Graphs\n",
        "        self.u0_pred, self.v0_pred, _ , _ = self.net_uv(self.x0_tf, self.t0_tf)\n",
        "        self.u_lb_pred, self.v_lb_pred, self.u_x_lb_pred, self.v_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf)\n",
        "        self.u_ub_pred, self.v_ub_pred, self.u_x_ub_pred, self.v_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf)\n",
        "        self.f_u_pred, self.f_v_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf)\n",
        "\n",
        "        # Loss\n",
        "        self.loss = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v0_tf - self.v0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_lb_pred - self.v_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_x_lb_pred - self.v_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_v_pred))\n",
        "\n",
        "        # Optimizers\n",
        "        # tf.contrib.opt.\n",
        "        self.optimizer = ScipyOptimizerInterface(self.loss,\n",
        "                                                                method = 'L-BFGS-B',\n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
        "\n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "\n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "    def net_uv(self, x, t):\n",
        "        X = tf.concat([x,t],1)\n",
        "\n",
        "        uv = self.neural_net(X, self.weights, self.biases)\n",
        "        u = uv[:,0:1]\n",
        "        v = uv[:,1:2]\n",
        "\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "\n",
        "        return u, v, u_x, v_x\n",
        "\n",
        "    def net_f_uv(self, x, t):\n",
        "        u, v, u_x, v_x = self.net_uv(x,t)\n",
        "\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "\n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "\n",
        "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
        "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u\n",
        "\n",
        "        return f_u, f_v\n",
        "\n",
        "    def callback(self, loss):\n",
        "        print('Loss:', loss)\n",
        "\n",
        "    def train(self, nIter):\n",
        "\n",
        "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
        "                   self.u0_tf: self.u0, self.v0_tf: self.v0,\n",
        "                   self.x_lb_tf: self.x_lb, self.t_lb_tf: self.t_lb,\n",
        "                   self.x_ub_tf: self.x_ub, self.t_ub_tf: self.t_ub,\n",
        "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}\n",
        "\n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' %\n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "\n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict = tf_dict,\n",
        "                                fetches = [self.loss],\n",
        "                                loss_callback = self.callback)\n",
        "\n",
        "\n",
        "    def predict(self, X_star):\n",
        "\n",
        "        tf_dict = {self.x0_tf: X_star[:,0:1], self.t0_tf: X_star[:,1:2]}\n",
        "\n",
        "        u_star = self.sess.run(self.u0_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v0_pred, tf_dict)\n",
        "\n",
        "\n",
        "        tf_dict = {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]}\n",
        "\n",
        "        f_u_star = self.sess.run(self.f_u_pred, tf_dict)\n",
        "        f_v_star = self.sess.run(self.f_v_pred, tf_dict)\n",
        "\n",
        "        return u_star, v_star, f_u_star, f_v_star"
      ],
      "metadata": {
        "id": "_LZZF5VR9p0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "noise = 0.0\n",
        "\n",
        "    # Doman bounds\n",
        "lb = np.array([-5.0, 0.0])\n",
        "ub = np.array([5.0, np.pi/2])\n",
        "\n",
        "N0 = 50\n",
        "N_b = 50\n",
        "N_f = 20000\n",
        "layers = [2, 100, 100, 100, 100, 2]"
      ],
      "metadata": {
        "id": "lXQH38q-9zJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# mount your Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# adjust the path to the location where the models will be written / read\n",
        "root_path = '/content/drive/My Drive/Dataset/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bivmnrC8aLJF",
        "outputId": "af378e9e-1608-47c0-8640-2e85303a8117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "2iY5FGpDasV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "data = scipy.io.loadmat('/content/drive/My Drive/Dataset/NLS.mat')\n",
        "\n",
        "t = data['tt'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = data['uu']\n",
        "Exact_u = np.real(Exact)\n",
        "Exact_v = np.imag(Exact)\n",
        "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact_u.T.flatten()[:,None]\n",
        "v_star = Exact_v.T.flatten()[:,None]\n",
        "h_star = Exact_h.T.flatten()[:,None]\n",
        "\n",
        "    ###########################\n",
        "\n",
        "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "x0 = x[idx_x,:]\n",
        "u0 = Exact_u[idx_x,0:1]\n",
        "v0 = Exact_v[idx_x,0:1]\n",
        "\n",
        "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "tb = t[idx_t,:]\n",
        "\n",
        "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
        "\n",
        "start_time = time.time()\n",
        "model.train(50000)\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "\n",
        "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
        "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error v: %e' % (error_v))\n",
        "print('Error h: %e' % (error_h))\n",
        "\n",
        "\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
        "H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
        "FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################\n",
        "\n",
        "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "\n",
        "fig, ax = plt.figure(1.0, 0.9)\n",
        "ax.axis('off')\n",
        "\n",
        "    ####### Row 0: h(t,x) ##################\n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "h = ax.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu',\n",
        "                  extent=[lb[1], ub[1], lb[0], ub[0]],\n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax.plot(t[125]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "leg = ax.legend(frameon=False, loc = 'best')\n",
        "#    plt.setp(leg.get_texts(), color='w')\n",
        "ax.set_title('$|h(t,x)|$', fontsize = 10)\n",
        "\n",
        "    ####### Row 1: h(t,x) slices ##################\n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "\n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact_h[:,100], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,H_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "DsGHRuI6YKt0",
        "outputId": "a0b4c62e-8eb4-4fd9-8143-06d7e5aae091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-42acfedf6093>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mX_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mub\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysicsInformedNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-11d9b40c9fc7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x0, u0, v0, tb, X_f, layers, lb, ub)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# tf.contrib.opt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         self.optimizer = ScipyOptimizerInterface(self.loss, \n\u001b[0m\u001b[1;32m     66\u001b[0m                                                                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'L-BFGS-B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                                                 options = {'maxiter': 50000,\n",
            "\u001b[0;32m<ipython-input-25-a3a018e4433e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loss, var_list, equalities, inequalities, **optimizer_kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'variables' is not defined"
          ]
        }
      ]
    }
  ]
}