{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O62yXZ4QcRjk",
        "HIfJgv7Ad5xs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução às Redes Neurais (Dia 02) ![Picture1.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "rKJbDcsngBrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Introdução (Exemplo 1)"
      ],
      "metadata": {
        "id": "kN2yRrb2AM7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo**: construir uma rede neural para classificar dígitos manuscritos da base de dados MNIST.\n",
        "\n",
        "* Os dígitos são imagens em escala de cinza com dimensão $28\\times 28$ e distribuídas em 10 classes ($0,1,2,\\ldots,9$).\n",
        "\n",
        "* Imagens em escala de cinza são matrizes e cada entrada dela equivale a um nível de intensidade luminosa. Neste conjunto de dados, as imagens variam a intensidade luminosa de $0$ (preto) a $255$ (branco) apenas com entradas inteiras.\n",
        "\n",
        "* Será então construída uma rede neural cuja entrada são as $784=28\\times 28$ entradas das figuras e a saída é a classe em que esta imagem deve estar contida. Neste tipo de situação as matrizes que representam as imagens serão reescritas como um vetor, de tal forma que as entradas do vetor correspondem as colunas da matriz da imagem.\n",
        "\n",
        "* Esta rede terá duas camadas, além da entrada, com 10 neurônios, na primeira utilizaremos a função de ativação `ReLU` e na segunda camada a função `softmax`.\n",
        "\n",
        "* A classe numérica em que cada imagem está contida deverá ser convertida para o que se chama de `formato categórico`, isto é, a cada classe será associado um vetor em $\\mathbb{R}^{10}.$ Por exemplo, a classe $0$ será associada ao vetor $(1,0,0,\\ldots,0),$ a classe $1$ ao vetor $(0,1,0,\\ldots,0)$ e assim por diante. Isto é necessário para podermos comparar adequadamente a saída da rede com a classe, pois estamos calculando distância entre vetores.\n",
        "\n",
        "* Aqui utilizaremos como função de perda a entropia cruzada (*cross-entropy)* $$\\mathcal{L}(y,\\hat{y}_\\theta)=-\\sum_{j=1}^{10} y_j\\ln\\hat{y}_{\\theta,j},$$ sendo $y_j$ a classe original do dígito, $\\hat{y}_{\\theta,j}$ a classe estimada pela rede e $j$ é o índice das imagens utilizadas no treinamento.\n",
        "\n",
        "* A saída que será produzida pela rede neural é um vetor de probabilidades, uma vez que a função de ativação escolhida para a saída é a `softmax`. Isto é, dada uma imagem $\\mathbf{x}$ de entrada, que deverá ser classificada, a rede produzirá como resposta, por exemplo, um vetor na forma $\\hat{y}_{\\theta,j}(\\mathbf{x})=(0.05 \\, ,\\,0.07 \\, ,\\, 0.01\\, ,\\, 0.12\\, ,\\, 0.57\\, ,\\, 0.02\\, ,\\, 0.015\\, ,\\, 0.015\\, ,\\, 0.05\\, ,\\, 0.08).$ Nesta situação, a imagem $\\mathbf{x}$ será classificada como um $4$.\n",
        "\n",
        "*OBS: o código base é de autoria de Samson Zhang (https://www.samsonzhang.com/building)*"
      ],
      "metadata": {
        "id": "XpDTOK6YgkJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 1: manipulação e visualização dos dados"
      ],
      "metadata": {
        "id": "d4KDxgiWqT4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Iniciamos importando as bibliotecas necessárias!**\n",
        "\n",
        "`numpy` para cálculo numérico\n",
        "\n",
        "`panda` para manipulação de dados\n",
        "\n",
        "`pyplot` para imprimir as figuras"
      ],
      "metadata": {
        "id": "eEFdBfy7aE3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTyOvdfQZNb_"
      },
      "outputs": [],
      "source": [
        "# bibliotecas básicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui estamos lendo o arquivo que contém a informação das imagens.\n",
        "\n",
        "*obs: caso esteja utilizando o `google colab` espere carregar todo o arquivo `train.csv`, caso contrário a rede dará algum erro e não convergirá adequadamente.*"
      ],
      "metadata": {
        "id": "5axnyzSSe-gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# le o arquivo das imagens\n",
        "data = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "tQtT5Ubxa74_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Estamos lidando com 42000 amostras de imagens de manuscritos de tamanho $28\\times 28=784$ já escritos no formato de vetor.\n",
        "\n",
        "Perceba que o vetor tem uma entrada a mais. Ela representa a etiqueta daquele número manuscrito."
      ],
      "metadata": {
        "id": "NsZ_800TbI5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)\n",
        "m, n = data.shape # método que devolve a dimensão dos dados\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLU_FlRZfNYZ",
        "outputId": "f3d885c2-f7f2-4ea7-da15-56969cd32e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Das 42000 imagens, separamos 4200 (10%) delas pra teste e as demais para o treino!"
      ],
      "metadata": {
        "id": "Kfme66_2b64M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separar o que é teste e o que é treino\n",
        "\n",
        "np.random.shuffle(data) # começamos embaralhando os dados com o método shuffle\n",
        "\n",
        "#teste\n",
        "data_dev = data[0:4200].T\n",
        "Y_dev = data_dev[0] # classe\n",
        "X_dev = data_dev[1:n] # a informação das imagens\n",
        "\n",
        "#treino\n",
        "data_train = data[4200:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "_,m_train = X_train.shape"
      ],
      "metadata": {
        "id": "9S0NRqaCfn1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Vamos analisar como são as imagens. Perceba que nas matrizes os valores são inteiros e variam entre $0$ e $255$."
      ],
      "metadata": {
        "id": "-fjuof-7nnQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mostra as imagens\n",
        "Z = X_train.reshape((28,28,m_train)) # o método reshape serve para transformar o vetor empilhado de volta numa matriz\n",
        "\n",
        "print(Z[:,:,0]) # começamos mostrando como são os dados das imagens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cNQFv8QgrIX",
        "outputId": "e933857e-8ac4-4827-a354-eff34ef4a807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198\n",
            "  253 129  10   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 197\n",
            "  252 253  84   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  19  85  47   0   0   0   0   0   0 197\n",
            "  252 253  84   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  26 231 252 240 101   0   0   0   0   0 197\n",
            "  252 241  47   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 104 253 253 253 101   0   0   0   0   0 198\n",
            "  253 226   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 101 253 252 252 177   0   0   0   0   0   0 197\n",
            "  252 225   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0  29 234 253 252 186  43   0   0   0   0   0  45 240\n",
            "  252 225   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0  85 252 253 227  31   0   0   0   0   0   0  57 252\n",
            "  252 225   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 163 255 228  53  29  29  29  29  29  13  70 253\n",
            "  253 226   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  38 146 234 252 252 253 252 252 252 207 225 252\n",
            "  252 200   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  38 143 205 253 252 252 252 253 252 252\n",
            "  252 113   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  13  28  28  28  28  53 252 252\n",
            "  252 113   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  41 253 253\n",
            "  190   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 166 252 252\n",
            "  139   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  38 253 252 252\n",
            "   52   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 113 253 252 127\n",
            "    3   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 176 254 222  25\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  57 243 247 103   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 123 252 225   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  97 252  75   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Agora vemos como os dígitos manuscritos são visualmente."
      ],
      "metadata": {
        "id": "GZBckKt9oAEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "\n",
        "ax.imshow(Z[:,:,0],cmap='gray') # plota a imagem\n",
        "ax.set_title(f\"Rótulo: {Y_train[0]}\") # inclui o rótulo em que essa imagem está classificada\n",
        "ax.get_xaxis().set_visible(False) # tiram os eixos x e y da visualização\n",
        "ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "yL-TrJKjn_d0",
        "outputId": "b8dff5de-a994-4a9f-f7ec-a67a5a8e26f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui iremos dividir as entradas dos vetores de teste e treino por $255$, isto é, estamos fazendo o que é conhecido de normalização. Na prática isto não é uma normalização dos vetores das imagens, porém agora a escala de como a cor é vista irá variar entre $0$ e $1$, e não mais entre $0$ e $255$. Este tipo de cuidado é importante para ajudar a rede a convergir evitando que o gradiente tenha a possibilidade de crescer demais."
      ],
      "metadata": {
        "id": "6hjcpEkwmNqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dev = X_dev / 255. #teste\n",
        "X_train = X_train / 255. #treino"
      ],
      "metadata": {
        "id": "C4nPYMw4mNVu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "0c5117b7-9497-4f96-f049-27214b9cd450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_dev' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c78ba1e6a7d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m \u001b[0;31m#teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m \u001b[0;31m#treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_dev' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 2: construção da rede neural"
      ],
      "metadata": {
        "id": "GT2lkeMQqNkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui definimos as funções de ativação que serão utilizadas: `ReLU` e `softmax`.\n",
        "\n",
        "Lembrando que:\n",
        "1. $\\mathrm{ReLU}(x)=\\max(0,x)$;\n",
        "2. $\\frac{d}{dx} \\mathrm{ReLU}(x)=1$, se $x>0$, e $\\frac{d}{dx} \\mathrm{ReLU}(x)=0$, se $x<0$;\n",
        "3. `softmax` produz um vetor em que cada entrada é dada por $\\sigma(\\mathbf{z})_j=\\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}.$"
      ],
      "metadata": {
        "id": "2FSTMEq066qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definicao das funções de ativação\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0 # retorna 1 se Z > 0  e retorna 0 se Z < 0\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
        "    return A"
      ],
      "metadata": {
        "id": "IVT23iksiS1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Construímos aqui as operações nas camadas.\n",
        "\n",
        "1. Temos que $X$ é a matriz que contém as informações de cada imagem. Cada coluna desta matriz corresponde a uma imagem de um dígito. Essa matriz tem dimensão $784\\times m$, $m$ para nós é a quantidade de imagens de treino. Neste caso, estamos usando $m=38800$.\n",
        "\n",
        "2. Daí, fazemos a transformação afim das entradas\n",
        "$Z^{[1]}=W^{[1]}X+b^{[1]},$\n",
        "sendo que a matriz $W^{[1]}$ contém as informações dos pesos que ligam as entradas aos  $10$ neurônios da primeira camada e $b^{[1]}$ corresponde aos viéses de cada neurônio da primeira cadama. O resultado dessa operação fornece um vetor $10\\times m.$\n",
        "\n",
        "3. Em seguida, temos que aplicar a função de ativação para os neurônios da primeira camada fazendo $A^{[1]}=\\mathrm{ReLU}(Z^{[1]}).$ Note que essa operação é feita entrada à entrada da matriz $Z^{[1]}$ e, portanto, a dimensão de $A^{[1]}$ é a mesma de $Z^{[1]}$.\n",
        "\n",
        "4. Para a segunda camada, onde será produzida a saída, repetimos o procedimento descrito no item 2. Fazemos novamente uma transformação afim das entradas. No entanto, aqui as entradas correspondem às saídas dos neurônios anteriores. Portanto, a matriz de pesos aqui precisa ter dimensão menor, no caso $10\\times 10$. Assim, a transformação afim é $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}.$ Novamente, aqui $Z^{[2]}$ produzirá uma matriz $10\\times m.$\n",
        "\n",
        "5. Finalmente, precisamos aplicar a função de ativação na transformação afim que ocorreu na segunda camada. Neste caso a transformação será uma `softmax` que corresponderá as probabilidades citadas iniciais. Assim, $A^{[2]}=\\mathrm{softmax}(Z^{[2]}).$"
      ],
      "metadata": {
        "id": "U9Q_zv3Uc0R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos operar as imagens nas camadas\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "_zl86jqgi4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Definindo os parâmetros iniciais: pesos e viéses\n",
        "\n",
        "O método `rand(a,b)` produz uma matriz de dimensões $a\\times b$ com valores aleatórios que são amostrados da distribuição uniforme no intervalo $[0,1]$. Fazendo, por exemplo, `np.random.rand(10, 784) - 0.5` iremos gerar uma matriz $10\\times 784$ com entradas no intervalo $[-0.5\\, , \\, 0.5].$\n",
        "\n",
        "Na prática a escolha inicial de pesos e viéses é feita usando método especializados. Tanto o Keras/TensorFlow como o Pytorch trazem as opções usuais de inicializadores para os pesos e viéses que variam desde as distribuições uniforme e normal até os inicializadores de Xavier Glorot. Este último, em particular, tem como objetivo inicializar os pesos de modo que a variância das ativações seja a mesma em todas as camadas. Essa variância constante ajuda a evitar que o gradiente exploda ou desapareça.\n",
        "\n",
        "*Referência: X. Glorot, Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 2010. URL: https://proceedings.mlr.press/v9/glorot10a.html*"
      ],
      "metadata": {
        "id": "O62yXZ4QcRjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definição dos parâmetros iniciais\n",
        "def init_params():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "mjsAGpM7gwWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Essa parte do código faz a codificação das classes das imagens para o formato categórico. Isto é importante para a comparação do resultado numérico gerado pela rede neural com os vetores de rótulos"
      ],
      "metadata": {
        "id": "lUuA7H3Bc_C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# faz o one-hot encoding do vetor de rótulos\n",
        "def one_hot(Y): # one-hot encoding\n",
        "    Y = Y.astype(int)\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y"
      ],
      "metadata": {
        "id": "D7LEmfb0jYPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Parte 2.1: cálculo das derivadas e o *backpropagation*"
      ],
      "metadata": {
        "id": "ucOLPFKsdf95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Agora entramos na parte mais complicada que é o cálculo do gradiente para a atualização dos parâmetros.\n",
        "\n",
        "O que pretendemos calcular é a derivada da função de perda com relação a cada parâmetro de peso e viés. Para um classificador `softmax`, usaremos uma função de perda de entropia cruzada:\n",
        "$$\\mathcal{L}(y,\\hat{y}_\\theta) = -\\frac{1}{m}\\sum_{i=1}^{10} y_i \\ln(\\hat{y}_{\\theta,i}).$$\n",
        "*Obs: é praxe dividir a função de perda pela quantidade de amostras que estão sendo utilizadas, no caso aqui $m$.*\n",
        "\n",
        "Aqui, $\\hat{y}_\\theta$ é o vetor dos rótulos previstos e ele pode ter uma forma como:\n",
        "$$\\begin{bmatrix} 0.01 \\ 0.02 \\ 0.05 \\ 0.02 \\ 0.80 \\ 0.01 \\ 0.01 \\ 0.00 \\ 0.01 \\ 0.07\\end{bmatrix}.$$\n",
        "\n",
        "O vetor $y$ é a codificação `one-hot` do rótulo do dado de treinamento. Se o rótulo para um exemplo de treinamento for 4, por exemplo, a sua codificação `one-hot` ficaria na forma:\n",
        "$$\\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}.$$\n",
        "\n",
        "Observe que na soma $$\\sum_{i=1}^{10} y_i \\ln(\\hat{y}_{\\theta,i}),$$ o valor de $y_i = 0,$ para todos os $i$ exceto o rótulo correto. A função de perda para uma dada amostra,  é apenas o logaritmo da probabilidade dada para a previsão estimada. Em nosso exemplo acima, $$\\mathcal{L}(y,\\hat{y}_\\theta) = -\\ln(\\hat{y}_5) = -\\ln(0.80) \\approx 0.2231.$$ Observe que, quanto mais próxima a probabilidade de predição estiver de 1, mais próxima a perda estará de 0. Conforme a probabilidade se aproxima de 0, a perda se aproxima de $+\\infty$.\n",
        "\n",
        "A minimização da função de perda melhora a precisão do nosso modelo. Como foi possível ver no exemplo, quanto mais perto a rede aproxima dos rótulos originais, mais perto de zero ficará a função de perda. O processo de otimização é feito através da descida do gradiente, em que subtraímos dos parâmetros que queremos otimizar ($W^{[1]}, W^{[2]}, b^{[1]}$ e $b^{[2]}$) uma quantidade proporcional ao gradiente da função de perda naquela variável, isto é:\n",
        "$$W^{[1]} := W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}$$\n",
        "$$ b^{[1]} := b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}$$\n",
        "$$ W^{[2]} := W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$$\n",
        "$$ b^{[2]} := b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}, $$\n",
        "o parâmetro $\\alpha$ é o que chamamos de taxa de aprendizagem (*learning rate*).\n",
        "\n",
        "Nosso objetivo no *backpropagation* é calcular as derivadas $\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}},\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}},\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$ e $\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}.$ Apenas para simplificar, escreveremos essas quantidades como $dW^{[1]}, db^{[1]}, dW^{[2]},$ e $db^{[2]}$. Esses valores são calculados usando a regra da cadeia retrocedendo em nossa rede, começando pelo cálculo de $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}$, ou $dZ^{[2]}$. Não é imediato, como veremos a seguir, mas essa derivada é dada por:\n",
        "$$dZ^{[2]} = \\frac{1}{m} \\left(A^{[2]}-y\\right).$$\n",
        "\n",
        "A partir desta derivada $dZ^{[2]}$, podemos utilizar a regra da cadeia, para $dW^{[2]}$ e $db^{[2]}$. Temos que:\n",
        "$dW^{[2]} = dZ^{[2]} A^{[1]T}$ e $db^{[2]} =  \\Sigma {dZ^{[2]}}.$\n",
        "\n",
        "Então, para calcular $dW^{[1]}$ e $db^{[1]}$, primeiro encontraremos $dZ^{[1]}$, que é dado por:\n",
        "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (Z^{[1]}).$$\n",
        "Em seguida, obtemos:\n",
        "$dW^{[1]} = dZ^{[1]} X^{T}$ e $db^{[1]} = \\Sigma {dZ^{[1]}}.$"
      ],
      "metadata": {
        "id": "RiwYCvz1Ku8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui vamos mostrar que a derivada $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}=\\frac{1}{m}\\left(A^{[2]}-y\\right).$\n",
        "\n",
        "Começamos, então, tirando o $\\ln$ do `softmax` para obter:\n",
        "\\begin{align*}\\ln\\hat{y}_{\\theta,i} & =\\ln\\left(\\frac{e^{z_{i}}}{\\sum_{j=1}^{10}e^{z_{j}}}\\right)\\\\\n",
        "& =\\ln\\left(e^{z_{i}}\\right)-\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)\\\\\n",
        "& =z_{i}-\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)\n",
        "\\end{align*}\n",
        "\n",
        "Em seguida derivamos a equação anterior em relação a variável $z_{k}$ para obtermos:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\frac{\\partial z_{i}}{\\partial z_{k}}-\\frac{\\partial\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}}\\end{align*}\n",
        "\n",
        "Note que o primeiro elemento de lado direito ficará na forma $$\\frac{\\partial z_{i}}{\\partial z_{k}}=\\begin{cases}\n",
        "1 & z_{i}=z_{k}\\\\\n",
        "0 & \\text{caso contrário}\n",
        "\\end{cases}=\\mathbb{1}(z_{i}=z_{k})=\\delta_{ik},$$ em que $\\mathbb{1}(z_{i}=z_{k})$ é a função indicadora, que, neste caso, também pode ser representada pelo $\\delta_{ik}$ delta de Kronecker.\n",
        "\n",
        "A segunda parcela do lado direito pode ser simplificada também, onde obtemos:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}} & =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\frac{\\partial\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}\\frac{\\partial e^{z_{j}}}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\frac{\\partial z_{j}}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\mathbb{1}(z_{j}=z_{k})\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\delta_{jk}\\\\\n",
        "& =\\frac{e^{z_{k}}}{\\sum_{j=1}^{10}e^{z_{j}}}\\\\\n",
        "& =\\hat{y}_{\\theta,k}\n",
        "\\end{align*}\n",
        "\n",
        "Concluímos então que:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\delta_{ik}-\\hat{y}_{\\theta,k}\\\\\n",
        "\\Rightarrow \\frac{1}{\\hat{y}_{\\theta,i}}\\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\delta_{ik}-\\hat{y}_{\\theta,k}\\\\\n",
        "\\Rightarrow \\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\hat{y}_{\\theta,i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right).\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Agora podemos diferenciar a entropia cruzada em relação a uma variável local $z_k$ do `softmax`.\n",
        "Sendo a entropia cruzada dada por\n",
        "$$\\mathcal{L}(y,\\hat{y}_{\\theta})  =-\\frac{1}{m}\\sum_{i=1}^{10}y_{i}\\ln\\hat{y}_{\\theta,i},$$\n",
        "então\n",
        "\\begin{align*}\n",
        "\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =-\\frac{1}{m}\\sum_{i=1}^{10}y_{i}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}}\\\\\n",
        "& =-\\frac{1}{m}\\sum_{i=1}^{10}\\frac{y_{i}}{\\hat{y}_{\\theta,i}}\\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}}\\\\\n",
        "& =-\\frac{1}{m}\\sum_{i=1}^{10}\\frac{y_{i}}{\\hat{y}_{\\theta,i}}\\hat{y}_{\\theta,i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right)\\\\\n",
        "& =-\\frac{1}{m}\\sum_{i=1}^{10}y_{i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right)\\\\\n",
        "& =-\\frac{1}{m}\\sum_{i=1}^{10}\\left(y_{i}\\delta_{ik}+y_{i}\\hat{y}_{\\theta,k}\\right).\n",
        "\\end{align*}\n",
        "Quando $i=k$ a primeira parcela do somatório anterior se tornará $y_{k}$, reduzindo a derivada da entropia cruzada a:\n",
        "\\begin{align*}\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =\\left(-y_{k}+\\hat{y}_{\\theta,k}\\sum_{i=1}^{10}y_{i}\\right).\\end{align*}\n",
        "Observando que $\\sum_{i=1}^{10}y_{i}=1$ já que $y$ é um vetor *one-hot*, obtemos então o resultado desejado:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =\\frac{1}{m}\\left(\\hat{y}_{\\theta,k}-y_{k}\\right).\n",
        "\\end{align*}\n",
        "\n",
        "Finalmente, na forma vetorizada (como será tratado pelo `numpy`), podemos simplesmente escrever o gradiente como\n",
        "\\begin{align*}\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z} & =\\left(\\hat{y}_{\\theta}-y\\right).\\end{align*}\n"
      ],
      "metadata": {
        "id": "f9IsexmVSRfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Finalmente, vamos mostrar como o $dW^{[2]}$ pode ser obtido.\n",
        "\n",
        "Temos que\n",
        "$$dW^{[2]}=\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}\\frac{\\partial z^{[2]}}{\\partial W^{[2]}}=dZ^{[2]}\\frac{\\partial z^{[2]}}{\\partial W^{[2]}}=dZ^{[2]}(A^{[1]})^{T}.$$\n",
        "\n",
        "As demais são obtidas de forma similar."
      ],
      "metadata": {
        "id": "54tIDXs8iEGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cálculo das derivadas\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ2 = (1 / m) * (A2 - one_hot_Y)\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    db2 = np.sum(dZ2, axis=1).reshape(-1, 1)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    db1 = np.sum(dZ1, axis=1).reshape(-1, 1)\n",
        "    return dW1, db1, dW2, db2\n"
      ],
      "metadata": {
        "id": "0tG6Wj8sjZU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Atualiza os parâmetros de acordo com as derivadas. Faz a atualização de acordo com o gradiente descendente."
      ],
      "metadata": {
        "id": "uuRSPDmhdjpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# atualização dos parâmetros\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 -= alpha * dW1\n",
        "    b1 -= alpha * db1\n",
        "    W2 -= alpha * dW2\n",
        "    b2 -= alpha * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "jYynRIIEk4i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Parte 2.2: definição da função que fará as épocas\n",
        "Efetivamente a função que faz o processo de cálculo da rede neural acontecer e atualiza os parâmetros sucessivamente"
      ],
      "metadata": {
        "id": "P36H_z9WduiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos fazer a mágica acontecer!\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 50 == 0:\n",
        "            print(\"Iteração: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "t44TOFJXl7iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 3: treinamento e análise dos resultados"
      ],
      "metadata": {
        "id": "2hf_bYIMqZye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Finalmente realiza o treinamento!"
      ],
      "metadata": {
        "id": "HIfJgv7Ad5xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos treinar\n",
        "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYzkXrtvnAfd",
        "outputId": "14c18d02-98d9-460a-fa7a-b3215f098957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteração:  0\n",
            "[8 7 8 ... 3 3 7] [4 2 8 ... 4 5 4]\n",
            "0.11335978835978835\n",
            "Iteração:  50\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.4216666666666667\n",
            "Iteração:  100\n",
            "[4 6 8 ... 6 3 4] [4 2 8 ... 4 5 4]\n",
            "0.6213492063492063\n",
            "Iteração:  150\n",
            "[4 6 8 ... 6 3 4] [4 2 8 ... 4 5 4]\n",
            "0.6985978835978836\n",
            "Iteração:  200\n",
            "[4 6 8 ... 6 3 4] [4 2 8 ... 4 5 4]\n",
            "0.7420899470899471\n",
            "Iteração:  250\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.7693650793650794\n",
            "Iteração:  300\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.7884126984126985\n",
            "Iteração:  350\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.8041005291005291\n",
            "Iteração:  400\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.816957671957672\n",
            "Iteração:  450\n",
            "[4 6 8 ... 6 5 4] [4 2 8 ... 4 5 4]\n",
            "0.827037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Funções para verificar a qualidade da nossa rede neural"
      ],
      "metadata": {
        "id": "xJHjqO1VeBNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos testar se o trem ficou bom!\n",
        "def make_predictions(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "    predictions = get_predictions(A2)\n",
        "    return predictions\n",
        "\n",
        "def test_prediction(index, W1, b1, W2, b2):\n",
        "    current_image = X_dev[:, index, None]\n",
        "    prediction = make_predictions(X_dev[:, index, None], W1, b1, W2, b2)\n",
        "    label = Y_dev[index]\n",
        "    print(\"Previsão: \", prediction)\n",
        "    print(\"Rótulo: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28))\n",
        "    plt.imshow(current_image,cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BkHK3dnHor9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Testa o quanto a nossa rede acerta!"
      ],
      "metadata": {
        "id": "B13Ye0_4eJr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = X_dev.shape\n",
        "\n",
        "test_prediction(np.random.randint(1, b), W1, b1, W2, b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "3icE-RtupKs5",
        "outputId": "9b3c242f-60b4-433e-fd61-321967094564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previsão:  [9]\n",
            "Rótulo:  2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}