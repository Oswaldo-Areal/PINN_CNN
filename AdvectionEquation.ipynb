{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quFm19a7bKhA",
        "outputId": "b79b237c-271a-4dbb-c220-6a352fe6d8ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7e6eb61c27f3>:7: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n",
            "  from jax.config import config\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax import random, grad, vmap, jit, hessian\n",
        "from jax.example_libraries import optimizers\n",
        "from jax.experimental.ode import odeint\n",
        "from jax.nn import relu, elu\n",
        "from jax.config import config\n",
        "#from jax.ops import index_update, index\n",
        "from jax.numpy import index_exp\n",
        "from jax import lax\n",
        "from jax.flatten_util import ravel_pytree\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from torch.utils import data\n",
        "from tqdm import trange, tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.interpolate import griddata\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generator\n",
        "class DataGenerator(data.Dataset):\n",
        "    def __init__(self, u, y, s,\n",
        "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
        "        'Initialization'\n",
        "        self.u = u\n",
        "        self.y = y\n",
        "        self.s = s\n",
        "\n",
        "        self.N = u.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.key = rng_key\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        self.key, subkey = random.split(self.key)\n",
        "        inputs, outputs = self.__data_generation(subkey)\n",
        "        return inputs, outputs\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def __data_generation(self, key):\n",
        "        'Generates data containing batch_size samples'\n",
        "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
        "        s = self.s[idx,:]\n",
        "        y = self.y[idx,:]\n",
        "        u = self.u[idx,:]\n",
        "        # Construct batch\n",
        "        inputs = (u, y)\n",
        "        outputs = s\n",
        "        return inputs, outputs"
      ],
      "metadata": {
        "id": "YlZGapQPdHrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MLP\n",
        "def MLP(layers, activation=relu):\n",
        "  ''' Vanilla MLP'''\n",
        "  def init(rng_key):\n",
        "      def init_layer(key, d_in, d_out):\n",
        "          k1, k2 = random.split(key)\n",
        "          glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
        "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
        "          b = np.zeros(d_out)\n",
        "          return W, b\n",
        "      key, *keys = random.split(rng_key, len(layers))\n",
        "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
        "      return params\n",
        "  def apply(params, inputs):\n",
        "      for W, b in params[:-1]:\n",
        "          outputs = np.dot(inputs, W) + b\n",
        "          inputs = activation(outputs)\n",
        "      W, b = params[-1]\n",
        "      outputs = np.dot(inputs, W) + b\n",
        "      return outputs\n",
        "  return init, apply"
      ],
      "metadata": {
        "id": "CutUjeeTdJvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define modified MLP\n",
        "def modified_MLP(layers, activation=relu):\n",
        "  def xavier_init(key, d_in, d_out):\n",
        "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
        "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
        "      b = np.zeros(d_out)\n",
        "      return W, b\n",
        "\n",
        "  def init(rng_key):\n",
        "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
        "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
        "      def init_layer(key, d_in, d_out):\n",
        "          k1, k2 = random.split(key)\n",
        "          W, b = xavier_init(k1, d_in, d_out)\n",
        "          return W, b\n",
        "      key, *keys = random.split(rng_key, len(layers))\n",
        "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
        "      return (params, U1, b1, U2, b2)\n",
        "\n",
        "  def apply(params, inputs):\n",
        "      params, U1, b1, U2, b2 = params\n",
        "      U = activation(np.dot(inputs, U1) + b1)\n",
        "      V = activation(np.dot(inputs, U2) + b2)\n",
        "      for W, b in params[:-1]:\n",
        "          outputs = activation(np.dot(inputs, W) + b)\n",
        "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n",
        "      W, b = params[-1]\n",
        "      outputs = np.dot(inputs, W) + b\n",
        "      return outputs\n",
        "  return init, apply"
      ],
      "metadata": {
        "id": "lztZ0szkdNga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class PI_DeepONet:\n",
        "    def __init__(self, branch_layers, trunk_layers):\n",
        "        # Network initialization and evaluation functions\n",
        "        self.branch_init, self.branch_apply = modified_MLP(branch_layers, activation=np.tanh)\n",
        "        self.trunk_init, self.trunk_apply = modified_MLP(trunk_layers, activation=np.tanh)\n",
        "\n",
        "        # Initialize\n",
        "        branch_params = self.branch_init(rng_key = random.PRNGKey(1234))\n",
        "        trunk_params = self.trunk_init(rng_key = random.PRNGKey(4321))\n",
        "        params = (branch_params, trunk_params)\n",
        "\n",
        "        # Use optimizers to set optimizer initialization and update functions\n",
        "        self.opt_init, \\\n",
        "        self.opt_update, \\\n",
        "        self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3,\n",
        "                                                                      decay_steps=2000,\n",
        "                                                                      decay_rate=0.9))\n",
        "        self.opt_state = self.opt_init(params)\n",
        "\n",
        "        # Used to restore the trained model parameters\n",
        "        _, self.unravel_params = ravel_pytree(params)\n",
        "\n",
        "        self.itercount = itertools.count()\n",
        "\n",
        "        # Loggers\n",
        "        self.loss_log = []\n",
        "        self.loss_bcs_log = []\n",
        "        self.loss_res_log = []\n",
        "\n",
        "    # Define DeepONet architecture\n",
        "    def operator_net(self, params, u, x, t):\n",
        "        branch_params, trunk_params = params\n",
        "        y = np.stack([x,t])\n",
        "        B = self.branch_apply(branch_params, u)\n",
        "        T = self.trunk_apply(trunk_params, y)\n",
        "        outputs = np.sum(B * T)\n",
        "        return   outputs\n",
        "\n",
        "    # Define PDE residual\n",
        "    def residual_net(self, params, u, x, t, ux):\n",
        "        s = self.operator_net(params, u, x, t)\n",
        "        s_t = grad(self.operator_net, argnums=3)(params, u, x, t)\n",
        "        s_x = grad(self.operator_net, argnums=2)(params, u, x, t)\n",
        "\n",
        "        res = s_t + ux * s_x\n",
        "        return res\n",
        "\n",
        "    # Define boundary loss\n",
        "    def loss_bcs(self, params, batch):\n",
        "        inputs, outputs = batch\n",
        "        u, y = inputs\n",
        "        # Compute forward pass\n",
        "        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n",
        "        # Compute loss\n",
        "        loss = np.mean((outputs.flatten() - s_pred)**2)\n",
        "        return loss\n",
        "\n",
        "    # Define residual loss\n",
        "    def loss_res(self, params, batch):\n",
        "        # Fetch data\n",
        "        inputs, outputs = batch\n",
        "        u, y  = inputs\n",
        "        # Compute forward pass\n",
        "        pred = vmap(self.residual_net, (None, 0, 0, 0, 0))(params, u, y[:,0], y[:,1], y[:,2])\n",
        "        # Compute loss\n",
        "        loss = np.mean((pred)**2)\n",
        "        return loss\n",
        "\n",
        "    # Define total loss\n",
        "    def loss(self, params, bcs_batch, res_batch):\n",
        "        loss_bcs = self.loss_bcs(params, bcs_batch)\n",
        "        loss_res = self.loss_res(params, res_batch)\n",
        "        loss = 100 * loss_bcs +  loss_res\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # Define a compiled update step\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def step(self, i, opt_state, bcs_batch, res_batch):\n",
        "        params = self.get_params(opt_state)\n",
        "        g = grad(self.loss)(params, bcs_batch, res_batch)\n",
        "        return self.opt_update(i, g, opt_state)\n",
        "\n",
        "    # Optimize parameters in a loop\n",
        "    def train(self, bcs_dataset, res_dataset, nIter = 10000):\n",
        "        bcs_data = iter(bcs_dataset)\n",
        "        res_data = iter(res_dataset)\n",
        "\n",
        "        pbar = trange(nIter)\n",
        "        # Main training loop\n",
        "        for it in pbar:\n",
        "            # Fetch data\n",
        "            bcs_batch= next(bcs_data)\n",
        "            res_batch = next(res_data)\n",
        "\n",
        "            self.opt_state = self.step(next(self.itercount), self.opt_state, bcs_batch, res_batch)\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                params = self.get_params(self.opt_state)\n",
        "\n",
        "                # Compute losses\n",
        "                loss_value = self.loss(params, bcs_batch, res_batch)\n",
        "                loss_bcs_value = self.loss_bcs(params, bcs_batch)\n",
        "                loss_res_value = self.loss_res(params, res_batch)\n",
        "\n",
        "                # Store losses\n",
        "                self.loss_log.append(loss_value)\n",
        "                self.loss_bcs_log.append(loss_bcs_value)\n",
        "                self.loss_res_log.append(loss_res_value)\n",
        "\n",
        "                # Print losses\n",
        "                pbar.set_postfix({'Loss': loss_value,\n",
        "                                  'loss_bcs' : loss_bcs_value,\n",
        "                                  'loss_physics': loss_res_value})\n",
        "\n",
        "    # Evaluates predictions at test points\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def predict_s(self, params, U_star, Y_star):\n",
        "        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n",
        "        return s_pred\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def predict_res(self, params, U_star, Y_star):\n",
        "        r_pred = vmap(self.residual_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n",
        "        return r_pred"
      ],
      "metadata": {
        "id": "QLfC1lXEdWHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use double precision to generate data (due to GP sampling)\n",
        "def RBF(x1, x2, params):\n",
        "    output_scale, lengthscales = params\n",
        "    diffs = np.expand_dims(x1 / lengthscales, 1) - \\\n",
        "            np.expand_dims(x2 / lengthscales, 0)\n",
        "    r2 = np.sum(diffs**2, axis=2)\n",
        "    return output_scale * np.exp(-0.5 * r2)\n",
        "\n",
        "\n",
        "# Deinfe initial and boundary conditions for advection equation\n",
        "# IC: f(x, 0)  = sin(pi x)\n",
        "# BC: g(0, t) = sin (pi t / 2)\n",
        "f = lambda x: np.sin(np.pi * x)\n",
        "g = lambda t: np.sin(np.pi * t/2)\n",
        "\n",
        "# Advection solver\n",
        "def solve_CVC(key, gp_sample, Nx, Nt, m, P):\n",
        "    # Solve u_t + a(x) * u_x = 0\n",
        "    # Wendroff for a(x)=V(x) - min(V(x)+ + 1.0, u(x,0)=f(x), u(0,t)=g(t)  (f(0)=g(0))\n",
        "    xmin, xmax = 0, 1\n",
        "    tmin, tmax = 0, 1\n",
        "\n",
        "    N = gp_sample.shape[0]\n",
        "    X = np.linspace(xmin, xmax, N)[:,None]\n",
        "    V = lambda x: np.interp(x, X.flatten(), gp_sample)\n",
        "\n",
        "    # Create grid\n",
        "    x = np.linspace(xmin, xmax, Nx)\n",
        "    t = np.linspace(tmin, tmax, Nt)\n",
        "    h = x[1] - x[0]\n",
        "    dt = t[1] - t[0]\n",
        "    lam = dt / h\n",
        "\n",
        "    # Compute advection velocity\n",
        "    v_fn = lambda x: V(x) - V(x).min() + 1.0\n",
        "    v =  v_fn(x)\n",
        "\n",
        "    # Initialize solution and apply initial & boundary conditions\n",
        "    u = np.zeros([Nx, Nt])\n",
        "    # u = index_update(u, index[0, :], g(t))\n",
        "    u = u.at[index_exp[0, :]].set(g(t))\n",
        "    # u = index_update(u, index[:, 0], f(x))\n",
        "    u = u.at[index_exp[:, 0]].set(f(t))\n",
        "\n",
        "    # Compute finite difference operators\n",
        "    a = (v[:-1] + v[1:]) / 2\n",
        "    k = (1 - a * lam) / (1 + a * lam)\n",
        "    K = np.eye(Nx - 1, k=0)\n",
        "    K_temp = np.eye(Nx - 1, k=0)\n",
        "    Trans = np.eye(Nx - 1, k=-1)\n",
        "    def body_fn_x(i, carry):\n",
        "        K, K_temp = carry\n",
        "        K_temp = (-k[:, None]) * (Trans @ K_temp)\n",
        "        K += K_temp\n",
        "        return K, K_temp\n",
        "    K, _ = lax.fori_loop(0, Nx-2, body_fn_x, (K, K_temp))\n",
        "    D = np.diag(k) + np.eye(Nx - 1, k=-1)\n",
        "\n",
        "    def body_fn_t(i, u):\n",
        "        b = np.zeros(Nx - 1)\n",
        "        # b = index_update(b, index[0], g(i * dt) - k[0] * g((i + 1) * dt))\n",
        "        b = b.at[index_exp[0]].set(g(i * dt) - k[0] * g((i + 1) * dt))\n",
        "        # u = index_update(u, index[1:, i + 1], K @ (D @ u[1:, i] + b))\n",
        "        u = u.at[index_exp[1:, i + 1]].set( K @ (D @ u[1:, i] + b))\n",
        "        return u\n",
        "    UU = lax.fori_loop(0, Nt-1, body_fn_t, u)\n",
        "\n",
        "    # Input sensor locations and measurements\n",
        "    xx = np.linspace(xmin, xmax, m)\n",
        "    u = v_fn(xx)\n",
        "    # Output sensor locations and measurements\n",
        "    idx = random.randint(key, (P,2), 0, max(Nx,Nt))\n",
        "    y = np.concatenate([x[idx[:,0]][:,None], t[idx[:,1]][:,None]], axis = 1)\n",
        "    s = UU[idx[:,0], idx[:,1]]\n",
        "\n",
        "    return (x, t, UU), (u, y, s)\n",
        "\n",
        "# Geneate training data corresponding to one input sample\n",
        "def generate_one_training_data(key, P, Q):\n",
        "\n",
        "    subkeys = random.split(key, 10)\n",
        "    # Generate a GP sample\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(xmin, xmax, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(subkeys[0], (N,)))\n",
        "\n",
        "    v_fn = lambda x: np.interp(x, X.flatten(), gp_sample)\n",
        "    u_fn = lambda x: v_fn(x) - v_fn(x).min() + 1.0\n",
        "\n",
        "    (x, t, UU), (u, y, s) = solve_CVC(subkeys[1], gp_sample, Nx, Nt, m, P)\n",
        "\n",
        "    x_bc1 = np.zeros((P // 2, 1))\n",
        "    x_bc2 = random.uniform(subkeys[2], shape = (P // 2, 1))\n",
        "    x_bcs = np.vstack((x_bc1, x_bc2))\n",
        "\n",
        "    t_bc1 = random.uniform(subkeys[3], shape = (P//2, 1))\n",
        "    t_bc2 = np.zeros((P//2, 1))\n",
        "    t_bcs = np.vstack([t_bc1, t_bc2])\n",
        "\n",
        "    u_train = np.tile(u, (P, 1))\n",
        "    y_train = np.hstack([x_bcs, t_bcs])\n",
        "\n",
        "    s_bc1 = g(t_bc1)\n",
        "    s_bc2 = f(x_bc2)\n",
        "    s_train =  np.vstack([s_bc1, s_bc2])\n",
        "\n",
        "    x_r = random.uniform(subkeys[4], shape=(Q,1), minval=xmin, maxval=xmax)\n",
        "    t_r = random.uniform(subkeys[5], shape=(Q,1), minval=tmin, maxval=tmax)\n",
        "    ux_r = u_fn(x_r)\n",
        "\n",
        "    u_r_train = np.tile(u, (Q,1))\n",
        "    y_r_train = np.hstack([x_r, t_r, ux_r])\n",
        "    s_r_train = np.zeros((Q, 1))\n",
        "\n",
        "    return u_train, y_train, s_train, u_r_train, y_r_train, s_r_train\n",
        "\n",
        "# Geneate test data corresponding to one input sample\n",
        "def generate_one_test_data(key, Nx, Nt, P):\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(xmin, xmax, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(key, (N,)))\n",
        "\n",
        "    (x, t, UU), (u, y, s) = solve_CVC(key, gp_sample, Nx, Nt, m, P)\n",
        "\n",
        "    XX, TT = np.meshgrid(x, t)\n",
        "\n",
        "    u_test = np.tile(u, (Nx*Nt,1))\n",
        "    y_test = np.hstack([XX.flatten()[:,None], TT.flatten()[:,None]])\n",
        "    s_test = UU.T.flatten()\n",
        "\n",
        "    return u_test, y_test, s_test\n",
        "\n",
        "# Geneate training data corresponding to N input sample\n",
        "def generate_training_data(key, N, P, Q):\n",
        "    config.update(\"jax_enable_x64\", True)\n",
        "    keys = random.split(key, N)\n",
        "    u_train, y_train, s_train, u_r_train, y_r_train, s_r_train = vmap(generate_one_training_data, (0, None, None))(keys, P, Q)\n",
        "\n",
        "    u_train = np.float32(u_train.reshape(N * P,-1))\n",
        "    y_train = np.float32(y_train.reshape(N * P,-1))\n",
        "    s_train = np.float32(s_train.reshape(N * P,-1))\n",
        "\n",
        "    u_r_train = np.float32(u_r_train.reshape(N * Q,-1))\n",
        "    y_r_train = np.float32(y_r_train.reshape(N * Q,-1))\n",
        "    s_r_train = np.float32(s_r_train.reshape(N * Q,-1))\n",
        "\n",
        "    config.update(\"jax_enable_x64\", False)\n",
        "    return u_train, y_train, s_train, u_r_train, y_r_train,  s_r_train\n",
        "\n",
        "# Geneate test data corresponding to N input sample\n",
        "def generate_test_data(key, N, Nx, Nt, P):\n",
        "\n",
        "    config.update(\"jax_enable_x64\", True)\n",
        "    keys = random.split(key, N)\n",
        "\n",
        "    u_test, y_test, s_test = vmap(generate_one_test_data, (0, None, None, None))(keys, Nx, Nt, P)\n",
        "\n",
        "    u_test = np.float32(u_test.reshape(N * Nx * Nt,-1))\n",
        "    y_test = np.float32(y_test.reshape(N * Nx * Nt,-1))\n",
        "    s_test = np.float32(s_test.reshape(N * Nx * Nt,-1))\n",
        "\n",
        "    config.update(\"jax_enable_x64\", False)\n",
        "    return u_test, y_test, s_test\n",
        "\n",
        "# Compute relative l2 error over N test samples.\n",
        "def compute_error(key, Nx, Nt, P):\n",
        "    u_test, y_test, s_test = generate_test_data(key, 1, Nx, Nt, P)\n",
        "    s_pred = model.predict_s(params, u_test, y_test)[:,None]\n",
        "    error_s = np.linalg.norm(s_test - s_pred, 2) / np.linalg.norm(s_test, 2)\n",
        "    return error_s"
      ],
      "metadata": {
        "id": "McndVK9Pdfmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "\n",
        "# GRF length scale\n",
        "length_scale = 0.2\n",
        "\n",
        "# Resolution of the solution\n",
        "Nx = 100\n",
        "Nt = 100\n",
        "\n",
        "# Computational domain\n",
        "xmin = 0.0\n",
        "xmax = 1.0\n",
        "\n",
        "tmin = 0.0\n",
        "tmax = 1.0\n",
        "\n",
        "N = 1000 # number of input samples\n",
        "m = Nx   # number of input sensors\n",
        "P_train = 200   # number of output sensors, 100 for each side\n",
        "Q_train = 2000  # number of collocation points for each input sample\n",
        "\n",
        "# Generate training data\n",
        "u_bcs_train, y_bcs_train, s_bcs_train, u_res_train, y_res_train, s_res_train = generate_training_data(key, N, P_train, Q_train)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "branch_layers = [m, 100, 100, 100, 100, 100, 100]\n",
        "trunk_layers =  [2, 100, 100, 100, 100, 100, 100]\n",
        "\n",
        "model = PI_DeepONet(branch_layers, trunk_layers)\n",
        "\n",
        "\n",
        "# Create data set\n",
        "batch_size = 10000\n",
        "bcs_dataset = DataGenerator(u_bcs_train, y_bcs_train, s_bcs_train, batch_size)\n",
        "res_dataset = DataGenerator(u_res_train, y_res_train, s_res_train, batch_size)\n",
        "\n",
        "\n",
        "# Train\n",
        "# Note: may meet OOM issue if use Colab. Please train this model on the server.\n",
        "model.train(bcs_dataset, res_dataset, nIter=300000)\n",
        "\n",
        "\n",
        "params = model.get_params(model.opt_state)\n",
        "\n",
        "# Save the trained model\n",
        "flat_params, _  = ravel_pytree(model.get_params(model.opt_state))\n",
        "np.save('adv_params.npy', flat_params)\n",
        "np.save('adv_loss_res.npy', model.loss_res_log)\n",
        "np.save('adv_loss_bcs.npy', model.loss_bcs_log)\n",
        "\n",
        "# Restore the trained model\n",
        "flat_params = np.load('adv_params.npy')\n",
        "params = model.unravel_params(flat_params)\n",
        "\n",
        "\n",
        "loss_bcs_log = model.loss_bcs_log\n",
        "loss_res_log = model.loss_res_log\n",
        "\n",
        "# Restore losses\n",
        "loss_bcs_log = np.load('adv_loss_bcs.npy')\n",
        "loss_res_log = np.load('adv_loss_res.npy')\n",
        "\n",
        "#Plot for loss function\n",
        "plt.figure(figsize = (6,5))\n",
        "plt.plot(loss_bcs_log, lw=2, label='bcs')\n",
        "plt.plot(loss_res_log, lw=2, label='res')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rRhcDuOedcK",
        "outputId": "33323b0b-9afc-41f1-d445-a9e41d992c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 5216/300000 [6:30:56<336:48:16,  4.11s/it, Loss=0.17888418, loss_bcs=0.00039519198, loss_physics=0.13936499]"
          ]
        }
      ]
    }
  ]
}